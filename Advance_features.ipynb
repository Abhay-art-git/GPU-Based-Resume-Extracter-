# Cell 3: Create resume_extractor.py with GPU optimizations
resume_extractor_code = '''
import os
import json
import re
import subprocess
from typing import List, Dict, Optional, Union
from datetime import datetime
from pathlib import Path

import fitz  # PyMuPDF
from PIL import Image
from pydantic import BaseModel, Field, validator
import ollama
from langchain.text_splitter import RecursiveCharacterTextSplitter

# GPU-optimized PaddleOCR initialization
try:
    from paddleocr import PaddleOCR
    # Use GPU for OCR
    ocr = PaddleOCR(
    use_angle_cls=True,
    lang='en',
    device='gpu',  # Correct parameter name
    det_db_thresh=0.3,
    det_db_box_thresh=0.6,
    rec_batch_num=6
)
    print("OCR initialized with GPU support")
except Exception as e:
    print(f"OCR initialization failed: {e}")
    ocr = None

# Force Ollama to use GPU
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
os.environ['OLLAMA_NUM_GPU'] = '999'  # Use all GPU layers

# Your existing Pydantic models
class Experience(BaseModel):
    company: str
    position: str
    start_date: Optional[str] = None
    end_date: Optional[str] = None
    location: Optional[str] = None
    responsibilities: List[str] = []

    @validator('start_date', 'end_date', pre=True)
    def parse_dates(cls, v):
        if v and v.lower() in ['present', 'current', 'now']:
            return 'Present'
        return v

class Education(BaseModel):
    institution: str
    degree: str
    field_of_study: Optional[str] = None
    start_date: Optional[str] = None
    end_date: Optional[str] = None
    gpa: Optional[str] = None
    location: Optional[str] = None

class PersonalInfo(BaseModel):
    name: str
    email: Optional[str] = None
    phone: Optional[str] = None
    location: Optional[str] = None
    linkedin: Optional[str] = None
    github: Optional[str] = None
    website: Optional[str] = None

class Skills(BaseModel):
    technical: List[str] = []
    soft: List[str] = []
    languages: List[str] = []
    tools: List[str] = []

class ResumeData(BaseModel):
    personal_info: PersonalInfo
    summary: Optional[str] = None
    experience: List[Experience] = []
    education: List[Education] = []
    skills: Skills = Skills()
    certifications: List[str] = []
    projects: List[Dict[str, str]] = []
    achievements: List[str] = []

class ResumeExtractor:
    def __init__(self, model_name: str = "mistral:7b-instruct"):
        """Initialize with GPU optimizations."""
        self.model_name = model_name
        print(f"Initializing ResumeExtractor with {model_name} on GPU...")

        # Monitor GPU usage
        self.monitor_gpu()

        # Ensure model is available
        self.ensure_model_available()

    def monitor_gpu(self):
        """Monitor GPU usage."""
        try:
            import GPUtil
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu = gpus[0]
                print(f"GPU: {gpu.name}")
                print(f"GPU Memory: {gpu.memoryUsed}/{gpu.memoryTotal}MB ({gpu.memoryUtil*100:.1f}%)")
                print(f"GPU Load: {gpu.load*100:.1f}%")
        except:
            pass

    def ensure_model_available(self):
        """Check if model is available, pull only if not present."""
        try:
            # Check if model exists
            result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)
            
            if self.model_name in result.stdout:
                print(f"âœ“ Model {self.model_name} found in cache")
                return
            
            # Test the model
            print(f"Testing {self.model_name}...")
            response = ollama.chat(
                model=self.model_name,
                messages=[{'role': 'user', 'content': 'test'}],
                options={'num_predict': 10}
            )
            print(f"âœ“ Model {self.model_name} is ready!")
            
        except:
            print(f"âŒ Model not found")
            print(f"ðŸ“¥ Downloading {self.model_name}...")
            print("â³ This will take 5-10 minutes (one time only)")
            ollama.pull(self.model_name)
            print(f"âœ“ Model {self.model_name} downloaded!")

    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """Extract text from PDF with GPU-accelerated OCR fallback."""
        text = ""
        pdf_document = fitz.open(pdf_path)

        for page_num in range(len(pdf_document)):
            page = pdf_document[page_num]

            # Try direct text extraction first
            page_text = page.get_text()

            # If no text found and OCR available, use GPU-accelerated OCR
            if len(page_text.strip()) < 50 and ocr:
                print(f"Using GPU OCR for page {page_num + 1}...")

                # Convert page to image
                pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))
                img_data = pix.tobytes("png")

                # Save temporarily
                temp_img_path = f"/tmp/temp_page_{page_num}.png"
                with open(temp_img_path, "wb") as f:
                    f.write(img_data)

                # GPU-accelerated OCR
                result = ocr.ocr(temp_img_path, cls=True)

                # Extract text
                for line in result:
                    if line:
                        for word_info in line:
                            page_text += word_info[1][0] + " "
                        page_text += "\\n"

                # Clean up
                os.remove(temp_img_path)

            text += page_text + "\\n"

        pdf_document.close()
        return self.clean_text(text)

    def clean_text(self, text: str) -> str:
        """Clean and normalize extracted text."""
        text = re.sub(r'\\s+', ' ', text)
        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)
        text = text.replace('|', 'I')
        text = re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', text)
        return text.strip()

    def create_extraction_prompt(self, resume_text: str) -> str:
        """Create optimized prompt for GPU processing."""
        # Limit text for faster GPU processing
        if len(resume_text) > 3000:
            resume_text = resume_text[:3000]

        prompt = f"""You are an expert resume parser. Extract ALL information from the following resume and return it in valid JSON format.

IMPORTANT: Return ONLY valid JSON, no explanations.

Resume text:
{resume_text}

Required JSON structure:
{{
    "personal_info": {{
        "name": "",
        "email": "",
        "phone": "",
        "location": "",
        "linkedin": "",
        "github": "",
        "website": ""
    }},
    "summary": "",
    "experience": [
        {{
            "company": "",
            "position": "",
            "start_date": "",
            "end_date": "",
            "location": "",
            "responsibilities": []
        }}
    ],
    "education": [
        {{
            "institution": "",
            "degree": "",
            "field_of_study": "",
            "start_date": "",
            "end_date": "",
            "gpa": "",
            "location": ""
        }}
    ],
    "skills": {{
        "technical": [],
        "soft": [],
        "languages": [],
        "tools": []
    }},
    "certifications": [],
    "projects": [
        {{
            "name": "",
            "description": "",
            "technologies": "",
            "link": ""
        }}
    ],
    "achievements": []
}}

Return ONLY the JSON:"""

        return prompt

    def extract_with_llm(self, text: str, max_retries: int = 3) -> Dict:
        """GPU-optimized LLM extraction."""
        prompt = self.create_extraction_prompt(text)

        for attempt in range(max_retries):
            try:
                print(f"GPU extraction attempt {attempt + 1}/{max_retries}...")

                # Monitor GPU before extraction
                self.monitor_gpu()

                # Call Ollama with GPU optimization
                response = ollama.chat(
                    model=self.model_name,
                    messages=[
                        {
                            'role': 'system',
                            'content': 'You are a precise resume parser. Always return valid JSON.'
                        },
                        {
                            'role': 'user',
                            'content': prompt
                        }
                    ],
                    options={
                        'temperature': 0.1,
                        'top_p': 0.9,
                        'num_predict': 4096,
                        'num_ctx': 4096,
                        'num_gpu': 999,      # Use all GPU layers
                        'main_gpu': 0,       # Use first GPU
                        'f16_kv': True,      # Use FP16 for faster inference
                        'num_thread': 8,     # CPU threads for non-GPU ops
                        'low_vram': False    # We have enough VRAM on T4
                    }
                )

                # Extract and parse JSON
                json_text = response['message']['content']
                json_text = self.clean_json_response(json_text)
                data = json.loads(json_text)

                print("âœ“ GPU extraction successful!")
                return data

            except json.JSONDecodeError as e:
                print(f"JSON parsing error on attempt {attempt + 1}: {e}")
                if attempt == max_retries - 1:
                    return self.fix_json_errors(json_text)
            except Exception as e:
                print(f"Extraction error on attempt {attempt + 1}: {e}")
                if attempt == max_retries - 1:
                    raise

    def clean_json_response(self, text: str) -> str:
        """Clean LLM response to extract valid JSON."""
        text = re.sub(r'```json\\s*', '', text)
        text = re.sub(r'```\\s*', '', text)

        start_idx = text.find('{')
        end_idx = text.rfind('}')

        if start_idx != -1 and end_idx != -1:
            return text[start_idx:end_idx + 1]

        return text

    def fix_json_errors(self, json_text: str) -> Dict:
        """Attempt to fix common JSON errors."""
        try:
            json_text = json_text.replace("'", '"')
            json_text = re.sub(r',\\s*}', '}', json_text)
            json_text = re.sub(r',\\s*]', ']', json_text)

            return json.loads(json_text)
        except:
            return {
                "personal_info": {"name": "Error parsing resume"},
                "experience": [],
                "education": [],
                "skills": {"technical": [], "soft": [], "languages": [], "tools": []}
            }

    def validate_and_enhance(self, data: Dict) -> ResumeData:
        """Validate extracted data and enhance with post-processing."""
        if 'personal_info' not in data:
            data['personal_info'] = {}

        if 'email' in data['personal_info']:
            email = data['personal_info']['email']
            if email and '@' not in email:
                data['personal_info']['email'] = None

        if 'experience' in data and data['experience']:
            data['experience'] = sorted(
                data['experience'],
                key=lambda x: x.get('end_date', 'Present') == 'Present',
                reverse=True
            )

        return ResumeData(**data)

    def extract_resume(self, pdf_path: str) -> ResumeData:
        """Main method to extract structured data from resume PDF."""
        import time
        start_time = time.time()

        print(f"\\nProcessing on GPU: {pdf_path}")
        print("="*50)

        # Step 1: Extract text
        print("Step 1: Extracting text from PDF...")
        text = self.extract_text_from_pdf(pdf_path)

        if len(text.strip()) < 100:
            raise ValueError("Could not extract sufficient text from PDF")

        # Step 2: Extract with GPU-accelerated LLM
        print("Step 2: Analyzing with AI model on GPU...")
        raw_data = self.extract_with_llm(text)

        # Step 3: Validate
        print("Step 3: Validating and structuring data...")
        resume_data = self.validate_and_enhance(raw_data)

        end_time = time.time()
        print(f"\\nâœ“ Extraction completed in {end_time - start_time:.2f} seconds on GPU!")
        print("="*50)

        return resume_data
'''

with open('resume_extractor.py', 'w') as f:
    f.write(resume_extractor_code)
print("âœ“ Created resume_extractor.py with GPU optimizations and model caching")
